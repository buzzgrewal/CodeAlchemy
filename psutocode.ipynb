{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10867162,"sourceType":"datasetVersion","datasetId":6751106}],"dockerImageVersionId":30920,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pandas nltk","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kzUKxOkfTHsm","outputId":"3172774d-e19d-4298-a251-8d4c3dfad4d2","trusted":true,"execution":{"iopub.status.busy":"2025-02-27T11:37:05.108434Z","iopub.execute_input":"2025-02-27T11:37:05.108805Z","iopub.status.idle":"2025-02-27T11:37:09.314320Z","shell.execute_reply.started":"2025-02-27T11:37:05.108783Z","shell.execute_reply":"2025-02-27T11:37:09.313484Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.3)\nRequirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.2.4)\nRequirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2025.1)\nRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from nltk) (1.17.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.4->pandas) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.4->pandas) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.4->pandas) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.4->pandas) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.4->pandas) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.4->pandas) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.22.4->pandas) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.22.4->pandas) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.22.4->pandas) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.22.4->pandas) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.22.4->pandas) (2024.2.0)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import nltk\nnltk.download('punkt')\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zRWcXgs7TJB1","outputId":"5a38aec9-498a-4e42-d9ab-a6bc99da7623","trusted":true,"execution":{"iopub.status.busy":"2025-02-27T11:37:09.315713Z","iopub.execute_input":"2025-02-27T11:37:09.315957Z","iopub.status.idle":"2025-02-27T11:37:11.042447Z","shell.execute_reply.started":"2025-02-27T11:37:09.315936Z","shell.execute_reply":"2025-02-27T11:37:11.041603Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"import math\nimport argparse\nimport os\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader","metadata":{"id":"0Avv6y4OTKhr","trusted":true,"execution":{"iopub.status.busy":"2025-02-27T11:37:11.044177Z","iopub.execute_input":"2025-02-27T11:37:11.044582Z","iopub.status.idle":"2025-02-27T11:37:11.048504Z","shell.execute_reply.started":"2025-02-27T11:37:11.044559Z","shell.execute_reply":"2025-02-27T11:37:11.047590Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"class Vocabulary:\n    def __init__(self, freq_threshold=1):\n        # Special tokens\n        self.freq_threshold = freq_threshold\n        self.itos = {0: \"<pad>\", 1: \"<sos>\", 2: \"<eos>\", 3: \"<unk>\"}\n        self.stoi = {\"<pad>\": 0, \"<sos>\": 1, \"<eos>\": 2, \"<unk>\": 3}\n        self.freqs = {}\n        self.idx = 4\n\n    def build_vocabulary(self, sentence_list):\n        for sentence in sentence_list:\n            for token in str(sentence).strip().split():\n                self.freqs[token] = self.freqs.get(token, 0) + 1\n        for token, freq in self.freqs.items():\n            if freq >= self.freq_threshold:\n                self.stoi[token] = self.idx\n                self.itos[self.idx] = token\n                self.idx += 1\n\n    def numericalize(self, text):\n        if text is None or (isinstance(text, float) and pd.isna(text)):\n            return []\n        text = str(text)\n        tokens = text.strip().split()\n        return [self.stoi.get(token, self.stoi[\"<unk>\"]) for token in tokens]","metadata":{"id":"9Ro8F8zsTRk2","trusted":true,"execution":{"iopub.status.busy":"2025-02-27T11:37:11.049983Z","iopub.execute_input":"2025-02-27T11:37:11.050254Z","iopub.status.idle":"2025-02-27T11:37:11.065716Z","shell.execute_reply.started":"2025-02-27T11:37:11.050227Z","shell.execute_reply":"2025-02-27T11:37:11.064923Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"class PseudocodeDataset(Dataset):\n    def __init__(self, filepath, src_vocab, trg_vocab):\n        self.data = pd.read_csv(filepath, sep='\\t')\n        self.src_vocab = src_vocab\n        self.trg_vocab = trg_vocab\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        src = self.data.iloc[idx][\"text\"]\n        trg = self.data.iloc[idx][\"code\"]\n        src_indices = [self.src_vocab.stoi[\"<sos>\"]] + self.src_vocab.numericalize(src) + [self.src_vocab.stoi[\"<eos>\"]]\n        trg_indices = [self.trg_vocab.stoi[\"<sos>\"]] + self.trg_vocab.numericalize(trg) + [self.trg_vocab.stoi[\"<eos>\"]]\n        return torch.tensor(src_indices), torch.tensor(trg_indices)","metadata":{"id":"nIwc8P7zTU1t","trusted":true,"execution":{"iopub.status.busy":"2025-02-27T11:37:11.066586Z","iopub.execute_input":"2025-02-27T11:37:11.066871Z","iopub.status.idle":"2025-02-27T11:37:11.080564Z","shell.execute_reply.started":"2025-02-27T11:37:11.066843Z","shell.execute_reply":"2025-02-27T11:37:11.079912Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def collate_fn(batch):\n    src_batch, trg_batch = zip(*batch)\n    src_pad = nn.utils.rnn.pad_sequence(src_batch, batch_first=True, padding_value=src_vocab.stoi[\"<pad>\"])\n    trg_pad = nn.utils.rnn.pad_sequence(trg_batch, batch_first=True, padding_value=trg_vocab.stoi[\"<pad>\"])\n    return src_pad, trg_pad","metadata":{"id":"28_5VD4wTXG9","trusted":true,"execution":{"iopub.status.busy":"2025-02-27T11:37:11.081371Z","iopub.execute_input":"2025-02-27T11:37:11.081679Z","iopub.status.idle":"2025-02-27T11:37:11.098226Z","shell.execute_reply.started":"2025-02-27T11:37:11.081651Z","shell.execute_reply":"2025-02-27T11:37:11.097587Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"class PositionalEncoding(nn.Module):\n    def __init__(self, d_model, dropout=0.3, max_len=5000): \n        super().__init__()\n        self.dropout = nn.Dropout(p=dropout)\n        pe = torch.zeros(max_len, d_model) \n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1) \n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0) \n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        x = x + self.pe[:, :x.size(1)]\n        return self.dropout(x)\n","metadata":{"id":"E8K8hEmDTZDQ","trusted":true,"execution":{"iopub.status.busy":"2025-02-27T11:37:11.099032Z","iopub.execute_input":"2025-02-27T11:37:11.099271Z","iopub.status.idle":"2025-02-27T11:37:11.112293Z","shell.execute_reply.started":"2025-02-27T11:37:11.099241Z","shell.execute_reply":"2025-02-27T11:37:11.111454Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"class TransformerSeq2Seq(nn.Module):\n    def __init__(self, src_vocab_size, trg_vocab_size, d_model=512, nhead=8,\n                 num_encoder_layers=3, num_decoder_layers=3, dim_feedforward=512, dropout=0.3):  # Increased dropout\n        super().__init__()\n        self.src_embedding = nn.Embedding(src_vocab_size, d_model)\n        self.trg_embedding = nn.Embedding(trg_vocab_size, d_model)\n        self.positional_encoding = PositionalEncoding(d_model, dropout)\n        self.transformer = nn.Transformer(d_model=d_model, nhead=nhead,\n                                          num_encoder_layers=num_encoder_layers,\n                                          num_decoder_layers=num_decoder_layers,\n                                          dim_feedforward=dim_feedforward, dropout=dropout)\n        self.fc_out = nn.Linear(d_model, trg_vocab_size)\n        self.d_model = d_model\n\n    def forward(self, src, trg):\n        src_mask = self.generate_square_subsequent_mask(src.size(1)).to(src.device)\n        trg_mask = self.generate_square_subsequent_mask(trg.size(1)).to(trg.device)\n        src_emb = self.positional_encoding(self.src_embedding(src) * math.sqrt(self.d_model))\n        trg_emb = self.positional_encoding(self.trg_embedding(trg) * math.sqrt(self.d_model))\n        src_emb = src_emb.transpose(0, 1)\n        trg_emb = trg_emb.transpose(0, 1)\n        output = self.transformer(src_emb, trg_emb, src_mask=src_mask, tgt_mask=trg_mask)\n        output = output.transpose(0, 1)  \n        return self.fc_out(output)\n\n    def generate_square_subsequent_mask(self, sz):\n        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n        return mask\n","metadata":{"id":"SPxT9tPSTdJZ","trusted":true,"execution":{"iopub.status.busy":"2025-02-27T11:37:11.113108Z","iopub.execute_input":"2025-02-27T11:37:11.113295Z","iopub.status.idle":"2025-02-27T11:37:11.124864Z","shell.execute_reply.started":"2025-02-27T11:37:11.113279Z","shell.execute_reply":"2025-02-27T11:37:11.124258Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"def train_epoch(model, dataloader, optimizer, criterion, device):\n    model.train()\n    epoch_loss = 0\n    for src, trg in dataloader:\n        src, trg = src.to(device), trg.to(device)\n        optimizer.zero_grad()\n        output = model(src, trg[:, :-1])\n        output_dim = output.shape[-1]\n        output = output.contiguous().view(-1, output_dim)\n        trg_target = trg[:, 1:].contiguous().view(-1)\n        loss = criterion(output, trg_target)\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        optimizer.step()\n        epoch_loss += loss.item()\n    return epoch_loss / len(dataloader)","metadata":{"id":"UyNQsmwfTdtI","trusted":true,"execution":{"iopub.status.busy":"2025-02-27T11:37:11.126738Z","iopub.execute_input":"2025-02-27T11:37:11.126927Z","iopub.status.idle":"2025-02-27T11:37:11.143230Z","shell.execute_reply.started":"2025-02-27T11:37:11.126911Z","shell.execute_reply":"2025-02-27T11:37:11.142114Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"def evaluate(model, dataloader, criterion, device):\n    model.eval()\n    epoch_loss = 0\n    with torch.no_grad():\n        for src, trg in dataloader:\n            src, trg = src.to(device), trg.to(device)\n            output = model(src, trg[:, :-1])\n            output_dim = output.shape[-1]\n            output = output.contiguous().view(-1, output_dim)\n            trg = trg[:, 1:].contiguous().view(-1)\n            loss = criterion(output, trg)\n            epoch_loss += loss.item()\n    return epoch_loss / len(dataloader)","metadata":{"id":"Ku0DFbWkTf7w","trusted":true,"execution":{"iopub.status.busy":"2025-02-27T11:37:11.144536Z","iopub.execute_input":"2025-02-27T11:37:11.144821Z","iopub.status.idle":"2025-02-27T11:37:11.158057Z","shell.execute_reply.started":"2025-02-27T11:37:11.144792Z","shell.execute_reply":"2025-02-27T11:37:11.157209Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"def translate_sentence(model, sentence, src_vocab, trg_vocab, device, max_len=50):\n    model.eval()\n    tokens = [src_vocab.stoi[\"<sos>\"]] + src_vocab.numericalize(sentence) + [src_vocab.stoi[\"<eos>\"]]\n    src_tensor = torch.LongTensor(tokens).unsqueeze(0).to(device)\n    with torch.no_grad():\n        src_mask = model.generate_square_subsequent_mask(src_tensor.size(1)).to(device)\n        src_emb = model.positional_encoding(model.src_embedding(src_tensor) * math.sqrt(model.d_model))\n        src_emb = src_emb.transpose(0, 1)\n        memory = model.transformer.encoder(src_emb, src_mask)\n    trg_indices = [trg_vocab.stoi[\"<sos>\"]]\n    for i in range(max_len):\n        trg_tensor = torch.LongTensor(trg_indices).unsqueeze(0).to(device)\n        trg_mask = model.generate_square_subsequent_mask(trg_tensor.size(1)).to(device)\n        trg_emb = model.positional_encoding(model.trg_embedding(trg_tensor) * math.sqrt(model.d_model))\n        trg_emb = trg_emb.transpose(0, 1)\n        with torch.no_grad():\n            output = model.transformer.decoder(trg_emb, memory, tgt_mask=trg_mask)\n            output = output.transpose(0, 1)\n            pred_token = output[:, -1, :].argmax(1).item()\n        trg_indices.append(pred_token)\n        if pred_token == trg_vocab.stoi[\"<eos>\"]:\n            break\n    trg_tokens = [trg_vocab.itos[idx] for idx in trg_indices]\n    return trg_tokens\n","metadata":{"id":"0IY9Nv2zTiQy","trusted":true,"execution":{"iopub.status.busy":"2025-02-27T11:37:11.158979Z","iopub.execute_input":"2025-02-27T11:37:11.159260Z","iopub.status.idle":"2025-02-27T11:37:11.177504Z","shell.execute_reply.started":"2025-02-27T11:37:11.159232Z","shell.execute_reply":"2025-02-27T11:37:11.176803Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"if __name__ == '__main__':\n    train_file = \"/kaggle/input/psu-to-code/train.csv\"\n    eval_file = \"/kaggle/input/psu-to-code/eval.csv\"\n    epochs = 30\n    batch_size = 256\n    save_path = \"transformer_model.pth\"\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(\"Using device:\", device)\n\n    train_df = pd.read_csv(train_file,sep='\\t')\n    train_df.dropna(inplace=True)\n    train_df.info()\n    src_sentences = train_df['text'].fillna(\"\").astype(str).tolist()\n    trg_sentences = train_df['code'].fillna(\"\").astype(str).tolist()\n\n    global src_vocab, trg_vocab\n    src_vocab = Vocabulary(freq_threshold=1)\n    src_vocab.build_vocabulary(src_sentences)\n    trg_vocab = Vocabulary(freq_threshold=1)\n    trg_vocab.build_vocabulary(trg_sentences)\n\n    train_dataset = PseudocodeDataset(train_file, src_vocab, trg_vocab)\n    eval_dataset = PseudocodeDataset(eval_file, src_vocab, trg_vocab)\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n    eval_loader = DataLoader(eval_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n\n    model = TransformerSeq2Seq(len(src_vocab.stoi), len(trg_vocab.stoi)).to(device)\n    criterion = nn.CrossEntropyLoss(ignore_index=src_vocab.stoi[\"<pad>\"])\n    optimizer = optim.Adam(model.parameters(), lr=0.0005)\n\n    for epoch in range(epochs):\n        train_loss = train_epoch(model, train_loader, optimizer, criterion, device)\n        eval_loss = evaluate(model, eval_loader, criterion, device)\n        print(f'Epoch: {epoch+1}, Train Loss: {train_loss:.3f}, Eval Loss: {eval_loss:.3f}')\n\n    torch.save({\n        'model_state_dict': model.state_dict(),\n        'src_vocab': src_vocab,\n        'trg_vocab': trg_vocab\n    }, save_path)\n    print(\"Model saved to\", save_path)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j5zNYSILTkdK","outputId":"21922dfe-82f6-4d2d-ec7d-00b5bdf138da","trusted":true,"execution":{"iopub.status.busy":"2025-02-27T11:37:11.178226Z","iopub.execute_input":"2025-02-27T11:37:11.178404Z","iopub.status.idle":"2025-02-27T13:41:21.238257Z","shell.execute_reply.started":"2025-02-27T11:37:11.178389Z","shell.execute_reply":"2025-02-27T13:41:21.237188Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n<class 'pandas.core.frame.DataFrame'>\nIndex: 181862 entries, 1 to 246083\nData columns (total 2 columns):\n #   Column  Non-Null Count   Dtype \n---  ------  --------------   ----- \n 0   text    181862 non-null  object\n 1   code    181862 non-null  object\ndtypes: object(2)\nmemory usage: 4.2+ MB\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 1, Train Loss: 2.186, Eval Loss: 1.443\nEpoch: 2, Train Loss: 1.407, Eval Loss: 1.236\nEpoch: 3, Train Loss: 1.162, Eval Loss: 1.175\nEpoch: 4, Train Loss: 1.002, Eval Loss: 1.134\nEpoch: 5, Train Loss: 0.878, Eval Loss: 1.114\nEpoch: 6, Train Loss: 0.781, Eval Loss: 1.109\nEpoch: 7, Train Loss: 0.713, Eval Loss: 1.115\nEpoch: 8, Train Loss: 0.645, Eval Loss: 1.123\nEpoch: 9, Train Loss: 0.594, Eval Loss: 1.146\nEpoch: 10, Train Loss: 0.551, Eval Loss: 1.140\nEpoch: 12, Train Loss: 0.484, Eval Loss: 1.139\nEpoch: 13, Train Loss: 0.457, Eval Loss: 1.162\nEpoch: 14, Train Loss: 0.434, Eval Loss: 1.145\nEpoch: 15, Train Loss: 0.418, Eval Loss: 1.137\nEpoch: 16, Train Loss: 0.399, Eval Loss: 1.152\nEpoch: 17, Train Loss: 0.387, Eval Loss: 1.140\nEpoch: 18, Train Loss: 0.372, Eval Loss: 1.140\nEpoch: 19, Train Loss: 0.362, Eval Loss: 1.169\nEpoch: 20, Train Loss: 0.353, Eval Loss: 1.145\nEpoch: 21, Train Loss: 0.344, Eval Loss: 1.111\nEpoch: 22, Train Loss: 0.335, Eval Loss: 1.149\nEpoch: 23, Train Loss: 0.325, Eval Loss: 1.190\nEpoch: 24, Train Loss: 0.319, Eval Loss: 1.146\nEpoch: 25, Train Loss: 0.311, Eval Loss: 1.133\nEpoch: 26, Train Loss: 0.305, Eval Loss: 1.158\nEpoch: 27, Train Loss: 0.298, Eval Loss: 1.117\nEpoch: 28, Train Loss: 0.294, Eval Loss: 1.138\nEpoch: 29, Train Loss: 0.289, Eval Loss: 1.164\nEpoch: 30, Train Loss: 0.284, Eval Loss: 1.168\nModel saved to transformer_model.pth\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"newSavePath=\"PsuToCode.pth\"\ntorch.save(model.state_dict(), newSavePath)\nprint(\"Model saved to\", newSavePath)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T13:41:21.239414Z","iopub.execute_input":"2025-02-27T13:41:21.239878Z","iopub.status.idle":"2025-02-27T13:41:21.723184Z","shell.execute_reply.started":"2025-02-27T13:41:21.239853Z","shell.execute_reply":"2025-02-27T13:41:21.722476Z"}},"outputs":[{"name":"stdout","text":"Model saved to PsuToCode.pth\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"import pickle\nwith open(\"src_vocab.pkl\", \"wb\") as f:\n    pickle.dump(src_vocab, f)\nwith open(\"trg_vocab.pkl\", \"wb\") as f:\n    pickle.dump(trg_vocab, f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T13:41:21.723923Z","iopub.execute_input":"2025-02-27T13:41:21.724205Z","iopub.status.idle":"2025-02-27T13:41:21.745440Z","shell.execute_reply.started":"2025-02-27T13:41:21.724183Z","shell.execute_reply":"2025-02-27T13:41:21.744880Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"import torch\nimport pandas as pd\nimport nltk\nimport pickle\nfrom torch.utils.data import DataLoader\nimport torch.nn.utils.rnn as rnn_utils\n\ndef load_vocab(filepath):\n    with open(filepath, \"rb\") as f:\n        vocab = pickle.load(f)\n    return vocab\n\ndef eval_collate_fn(batch):\n    # Each batch element is assumed to be a tuple: (src, trg)\n    srcs, trgs = zip(*batch)\n    # Pad the source sequences (adjust padding_value as needed)\n    padded_srcs = rnn_utils.pad_sequence(srcs, batch_first=True, \n                                         padding_value=src_vocab.stoi.get(\"<pad>\", 0))\n    return padded_srcs, trgs\n\ndef translate_batch(model, src_batch, src_vocab, trg_vocab, device, max_len=50):\n    \"\"\"\n    Batch decoding using greedy search.\n    Assumes the model.forward(src, trg) returns logits for the target sequence.\n    \"\"\"\n    batch_size = src_batch.size(0)\n    src_batch = src_batch.to(device)\n    \n    # Initialize target sequences with <sos> token.\n    trg_init = torch.LongTensor([trg_vocab.stoi[\"<sos>\"]] * batch_size).unsqueeze(1).to(device)\n    preds = trg_init\n\n    for _ in range(max_len):\n        # Forward pass: output shape assumed to be (batch_size, seq_len, vocab_size)\n        output = model(src_batch, preds)\n        # Get logits for the last time step and perform greedy selection.\n        next_token_logits = output[:, -1, :]\n        next_tokens = next_token_logits.argmax(dim=-1, keepdim=True)\n        preds = torch.cat([preds, next_tokens], dim=1)\n        # Stop if all sequences have generated an <eos> token.\n        if (next_tokens == trg_vocab.stoi[\"<eos>\"]).all():\n            break\n\n    # Convert token indices to strings, ignoring <sos> and truncating at <eos>.\n    batch_tokens = []\n    for seq in preds:\n        tokens = []\n        for token in seq:\n            token = token.item()\n            if token == trg_vocab.stoi[\"<sos>\"]:\n                continue\n            if token == trg_vocab.stoi[\"<eos>\"]:\n                break\n            tokens.append(trg_vocab.itos[token])\n        batch_tokens.append(tokens)\n    return batch_tokens\n\ndef calculate_bleu(model, dataloader, src_vocab, trg_vocab, device):\n    references = []\n    hypotheses = []\n    with torch.no_grad():\n        for src_batch, trgs in dataloader:\n            batch_pred_tokens = translate_batch(model, src_batch, src_vocab, trg_vocab, device)\n            for i, trg in enumerate(trgs):\n                # Convert target token indices to strings, skipping special tokens.\n                trg_tokens = [trg_vocab.itos[token.item()] for token in trg \n                              if token.item() not in {trg_vocab.stoi[\"<sos>\"], trg_vocab.stoi[\"<eos>\"]}]\n                references.append([trg_tokens])\n                pred_tokens = [tok for tok in batch_pred_tokens[i] if tok not in {\"<sos>\", \"<eos>\"}]\n                hypotheses.append(pred_tokens)\n    bleu = nltk.translate.bleu_score.corpus_bleu(references, hypotheses)\n    return bleu\n\nif __name__ == '__main__':\n    eval_file = \"/kaggle/input/psu-to-code/eval.csv\"     \n    model_path = \"/kaggle/working/PsuToCode.pth\"  \n    src_vocab_path = \"/kaggle/working/src_vocab.pkl\"\n    trg_vocab_path = \"/kaggle/working/trg_vocab.pkl\"\n    \n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(\"Using device:\", device)\n    \n    src_vocab = load_vocab(src_vocab_path)\n    trg_vocab = load_vocab(trg_vocab_path)\n    \n    # Initialize your model (ensure TransformerSeq2Seq is defined/imported)\n    model = TransformerSeq2Seq(len(src_vocab.stoi), len(trg_vocab.stoi)).to(device)\n    state_dict = torch.load(model_path, map_location=device)\n    model.load_state_dict(state_dict)\n    model.eval()\n    \n    eval_dataset = PseudocodeDataset(eval_file, src_vocab, trg_vocab)\n    eval_dataloader = DataLoader(eval_dataset, batch_size=32, collate_fn=eval_collate_fn, pin_memory=True)\n    \n    bleu_score = calculate_bleu(model, eval_dataloader, src_vocab, trg_vocab, device)\n    print(f\"BLEU Score on Evaluation Set: {bleu_score:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T14:56:35.996718Z","iopub.execute_input":"2025-02-27T14:56:35.997029Z","iopub.status.idle":"2025-02-27T15:03:49.103200Z","shell.execute_reply.started":"2025-02-27T14:56:35.997007Z","shell.execute_reply":"2025-02-27T15:03:49.102394Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n  warnings.warn(\n<ipython-input-24-484a31774c88>:88: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  state_dict = torch.load(model_path, map_location=device)\n","output_type":"stream"},{"name":"stdout","text":"BLEU Score on Evaluation Set: 0.5691\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"import torch\nimport pickle\nfrom torch.utils.data import DataLoader\nimport torch.nn.utils.rnn as rnn_utils\n\ndef load_vocab(filepath):\n    with open(filepath, \"rb\") as f:\n        vocab = pickle.load(f)\n    return vocab\n\ndef collate_fn(batch):\n    # Each batch element is assumed to be a tuple: (src, _)\n    srcs, _ = zip(*batch)\n    # Pad the source sequences; adjust padding_value as needed.\n    padded_srcs = rnn_utils.pad_sequence(srcs, batch_first=True, \n                                         padding_value=src_vocab.stoi.get(\"<pad>\", 0))\n    return padded_srcs\n\ndef translate_batch(model, src_batch, src_vocab, trg_vocab, device, max_len=50):\n    \"\"\"\n    Batch version of translation using the model's forward method.\n    The model is assumed to take (src, trg) as input, where `trg` is the\n    currently generated sequence. Greedy decoding is used.\n    \"\"\"\n    batch_size = src_batch.size(0)\n    src_batch = src_batch.to(device)\n    \n    # Initialize target sequences with <sos> token.\n    trg_init = torch.LongTensor([trg_vocab.stoi[\"<sos>\"]] * batch_size).unsqueeze(1).to(device)\n    preds = trg_init\n\n    for _ in range(max_len):\n        # Call the model's forward method with the source and current target sequence.\n        # The output should be of shape (batch_size, seq_len, vocab_size).\n        output = model(src_batch, preds)\n        \n        # Get the logits for the last time step.\n        next_token_logits = output[:, -1, :]\n        # Greedy decoding: select the token with the highest probability.\n        next_tokens = next_token_logits.argmax(dim=-1, keepdim=True)\n        preds = torch.cat([preds, next_tokens], dim=1)\n        \n        # Check if every sequence in the batch generated an <eos> token.\n        if (next_tokens == trg_vocab.stoi[\"<eos>\"]).all():\n            break\n\n    # Convert predicted token indices to tokens, ignoring <sos> and stopping at <eos>.\n    batch_tokens = []\n    for seq in preds:\n        tokens = []\n        for token in seq:\n            token = token.item()\n            if token == trg_vocab.stoi[\"<sos>\"]:\n                continue\n            if token == trg_vocab.stoi[\"<eos>\"]:\n                break\n            tokens.append(trg_vocab.itos[token])\n        batch_tokens.append(tokens)\n    return batch_tokens\n\nif __name__ == '__main__':\n    test_file = \"/kaggle/input/psu-to-code/test.csv\"             \n    model_path = \"/kaggle/working/PsuToCode.pth\"\n    src_vocab_path = \"/kaggle/working/src_vocab.pkl\"\n    trg_vocab_path = \"/kaggle/working/trg_vocab.pkl\"\n    output_file = \"test_predictions.txt\"\n    \n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(\"Using device:\", device)\n    \n    src_vocab = load_vocab(src_vocab_path)\n    trg_vocab = load_vocab(trg_vocab_path)\n    \n    # Initialize your model and load state dict.\n    model = TransformerSeq2Seq(len(src_vocab.stoi), len(trg_vocab.stoi)).to(device)\n    state_dict = torch.load(model_path, map_location=device)\n    model.load_state_dict(state_dict)\n    model.eval()\n    \n    test_dataset = PseudocodeDataset(test_file, src_vocab, trg_vocab)\n    test_dataloader = DataLoader(test_dataset, batch_size=32, \n                                 collate_fn=collate_fn, pin_memory=True)\n    predictions = []\n    \n    with torch.no_grad():\n        for src_batch in test_dataloader:\n            # Perform batch translation on GPU using the updated translate_batch function.\n            batch_pred_tokens = translate_batch(model, src_batch, src_vocab, trg_vocab, device)\n            for tokens in batch_pred_tokens:\n                predictions.append(\" \".join(tokens))\n    \n    with open(output_file, \"w\") as f:\n        for pred in predictions:\n            f.write(pred + \"\\n\")\n    print(\"Predictions saved to\", output_file)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T14:39:06.179548Z","iopub.execute_input":"2025-02-27T14:39:06.179878Z","iopub.status.idle":"2025-02-27T14:44:32.391180Z","shell.execute_reply.started":"2025-02-27T14:39:06.179850Z","shell.execute_reply":"2025-02-27T14:44:32.390265Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n  warnings.warn(\n<ipython-input-22-5c7090417052>:76: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  state_dict = torch.load(model_path, map_location=device)\n","output_type":"stream"},{"name":"stdout","text":"Predictions saved to test_predictions.txt\n","output_type":"stream"}],"execution_count":22}]}